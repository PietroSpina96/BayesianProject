<<<<<<< HEAD
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
y0
fda_clustering_mahalanobis <- function(n_clust, alpha, eig, toll,data){
n <- dim(data)[1]
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
vect_sample <- 1:n
y0[1] <- sample(vect_sample,1)
for (k in 2:n_clust) {
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
# vector of labels
c_lab <- rep(0,n)
# eigenvalues and eigenfunctions for the alpha-mahalanobis function
values <- eig$values
vectors <- eig$vectors
Mahalanobis_Distance <- matrix(0, nrow = n, ncol = n)
for (i in 1:n){
for (j in 1:n){
Mahalanobis_Distance[i,j] <- alpha_Mahalanobis(alpha,data[i,],data[j,],values,vectors)
}
}
# i-th unit belongs to cluster_k if the distance(centroid_k,i-th unit) is the smallest one
Maha_dis <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis[i,k] <- Mahalanobis_Distance[i,y0[k]]
}
index <- which.min(Maha_dis[i,])
c_lab[i] <- index
}
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = dim(data)[2])
for (k in 1:n_clust)
centroids_random[k,] <- data[y0[k],]
loss_value1 <- gibbs_loss(n_clust = n_clust, centroids = centroids_random, label = c_lab, data = data)
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = dim(data)[2])
for (k in 1:n_clust)
centroids_mean[k,] <- colMeans(data[which(c_lab=='k'),])
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
while(abs(loss_value1 - loss_value2) >= toll){
c_lab <- rep(0,n)
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,],values,vectors)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
loss_value1 <- loss_value2
for (k in 1:n_clust){
centroids_mean[k,] <- colMeans(data[which(c_lab=='k'),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
}
return(list("label" = c_lab, "centroids" = centroids_mean))
}
# Application on the simulated data
k <- 2
clust <- fda_clustering_mahalanobis(n_clust = k, alpha = alpha, eig = eig, toll = 1e-6, data = data)
loss_value1
n <- dim(data)[1]
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
nclust <- 2
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
n_clust <- 2
n <- dim(data)[1]
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
vect_sample <- 1:n
y0[1] <- sample(vect_sample,1)
for (k in 2:n_clust) {
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
# vector of labels
c_lab <- rep(0,n)
# eigenvalues and eigenfunctions for the alpha-mahalanobis function
values <- eig$values
vectors <- eig$vectors
Mahalanobis_Distance <- matrix(0, nrow = n, ncol = n)
for (i in 1:n){
for (j in 1:n){
Mahalanobis_Distance[i,j] <- alpha_Mahalanobis(alpha,data[i,],data[j,],values,vectors)
}
}
# i-th unit belongs to cluster_k if the distance(centroid_k,i-th unit) is the smallest one
Maha_dis <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis[i,k] <- Mahalanobis_Distance[i,y0[k]]
}
index <- which.min(Maha_dis[i,])
c_lab[i] <- index
}
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = dim(data)[2])
for (k in 1:n_clust)
centroids_random[k,] <- data[y0[k],]
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_random[k,] <- data[y0[k],]
}
loss_value1 <- gibbs_loss(n_clust = n_clust, centroids = centroids_random, label = c_lab, data = data)
loss_value1
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_mean[k,] <- colMeans(data[which(c_lab=='k'),])
}
View(centroids_mean)
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
loss_value2
setwd("C:/Users/pietr/Desktop/Bayesian Statistics/Progetto/dati/BayesianProject")
load('Simulated_WP.RData')
data<-data1
rm(data1)
#### Loss function ####
#centroids è una matrice avente n_clust righe e ogni riga è un centroide
=======
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T)
##
a=5
M <- 750 # truncation level   M=500
Y <- vector(length=M) ## beta proportions in the stick breaking construction
tau <-  vector(length=M) ## support points
V <- vector(length=M)
# Simulation
Y <- rbeta(M,1,a) #simulation of the beta rvs
tau <- rnorm(M,0,1) # tau_i iid from alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##summation of the weights V
V <- V/sum(V)  ## Rinormalization of the weights
oth <- order(tau)
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab="",ylab="",cex.axis=1.5)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
title("Total mass a=5")
gra=gray(1:100/100)
gra=rep(gra,10)
for(j in 1:50){   #for(j in 1:100){
set.seed(j)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T)
##
a=50
M <- 750 # livello di troncamento   M=500
Y <- vector(length=M) ##Il vettore V conterrà le v.a. beta della costruzione stick breaking
tau <-  vector(length=M) ## i punti di supporto della mpa processo di Dirichlet - PRIMA li abbiamo chiamati theta
V <- vector(length=M)
# Simulo
Y <- rbeta(M,1,a) #simulo le beta
tau <- rnorm(M,0,1) # simulo i tau in modo iid da alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##Valuto la somma dei V
V <- V/sum(V)  ## Rinormalizzo i V
oth <- order(tau)
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab="",ylab="",cex.axis=1.5)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
title("Total mass a=50")
gra=gray(1:100/100)
gra=rep(gra,10)
for(j in 1:50){    #for(j in 1:100){
set.seed(j)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T,xlab="",ylab="",cex.axis=1.5)
a=1
M <- 100 # livello di troncamento   M=30
Y <- vector(length=M) ##Il vettore V conterrà le v.a. beta della costruzione stick breaking
tau <-  vector(length=M) ## i punti di supporto della mpa processo di Dirichlet - PRIMA li abbiamo chiamati theta
V <- vector(length=M)
# Simulo
Y <- rbeta(M,1,a) #simulo le beta
tau <- rnorm(M,0,1) # simulo i tau in modo iid da alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##Valuto la somma dei V
V <- V/sum(V)
round(V,4)
n=10 ## sample size
theta<-vector(length=n)
index <-vector(length=n)
for(j in 1:n){
index[j] <- sample(1:M,size=1,prob=V)
theta[j]=tau[index[j]]
}
index
theta
unique(theta)
#########################################################################################
### We could simulate a sample from a Dirichlet process X_1,...,X_n by
### (i) first simulating a (truncated) stick-breaking trajectory of a Dirichlet process P
### (ii) then simulating X_1,...,X_n
### as ABOVE.
### However, we could use a DIRECT sampling scheme
### GENERALIZED POLYA URN
###################################################################################
a=1  #0.1, 1,  100
n=10 ## sample size ##n=2, 10, 100, 697
theta<-vector(length=n) #simulated values: I rename it theta instead than X
theta[1] <- rnorm(1) #first simulated value from alpha_0 = N(0,1)
for(j in 2:n){ ## from the second simulated value, we get
w0<- a/(j-1+a) ## a NEW observation from alpha_0 with probability w0
w1<-rep(1/(j-1+a),j-1) ## or an OLD observation, each of the old obs with prob w1
index <- sample(0:(j-1),size=1,prob=c(w0,w1))
# Sample the index from a discrete distribution with weights
#   contained in the vector prob, i.e. sample from the categorical distr.
if(index==0){
theta[j] <- rnorm(1) # NEW observation
}
else{
theta[j]=theta[index] # OLD observation
}
}
theta
unique(theta)
# K_n: number of UNIQUE values in the sample theta
k <- length(unique(theta))
k
k <- vector(length=5000)
set.seed(19)
for(i in 1:5000){ ##Sampling from the generalized Polya urn repeatedly
##################################
##################################
theta[1] <- rnorm(1)
for(j in 2:n){
w0<-a/(j-1+a)
w1<-rep(1/(j-1+a),j-1)
index <- sample(0:(j-1),size=1,prob=c(w0,w1))
if(index==0){
theta[j] <- rnorm(1)
}
else{
theta[j]=theta[index]
}
}
###########################
###########################
k[i] <- length(unique(theta))
}
k
ymax=max(table(k)/5000)+0.01
x11()
plot(table(k)/5000,ylim=c(0,ymax),ylab="")
title("Prior probability of the number of clusters")
#### NON chiudere la finestra grafica
### SIMULATION
set.seed(13)  # fisso il seme aleatorio
Y <- rbeta(M,1,a) #simulated values of the M rvs which are beta(1,a)-distributed
tau <- rnorm(M,0,1) # simulated tau_i, iid from alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod   ## V_i = Y_i \prod_{j=1}^{i-1}(1-Y_j)
## "spezzo i bastoncini" utilizzando le M v.a. Y_i prima simulate
print(sum(V))  ## Compute the sum of all weights so far
V <- V/sum(V)  ## Rinormalization of the weights (needed because truncation of the infinite series)
a=1 ##### TOTAL MASS parameter
M <- 1000 # truncation level   M=1000 o M=500
Y <- vector(length=M)    ## Y is the vector of the beta proportions in the stick-breaking
tau <-  vector(length=M) ## support points of the DP (theta_i at the blackboard)
V <- vector(length=M)    ## V is the vector of the weights of the support points tau_i
### SIMULATION
set.seed(13)  # fisso il seme aleatorio
Y <- rbeta(M,1,a) #simulated values of the M rvs which are beta(1,a)-distributed
tau <- rnorm(M,0,1) # simulated tau_i, iid from alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod   ## V_i = Y_i \prod_{j=1}^{i-1}(1-Y_j)
## "spezzo i bastoncini" utilizzando le M v.a. Y_i prima simulate
print(sum(V))  ## Compute the sum of all weights so far
V <- V/sum(V)  ## Rinormalization of the weights (needed because truncation of the infinite series)
x11()
curve(dnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,ylim=c(0,0.5),xlab=" ",ylab="",cex.axis=1.5) #mean parameter alpha_0 (in magenta)
abline(h=0,lty=2)
lines(tau,V,"h",lwd=3,col="red")
title("Weights and support points for a simulated trajectory of a DP")
x11()
curve(dnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,ylim=c(0,0.5),xlab="alpha_0",ylab="",cex.axis=1.5) abline(h=0,lty=2)
lines(tau,V,"h",lwd=3,col="red")
title("Weights and support points for a simulated trajectory of a DP")
x11()
curve(dnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,ylim=c(0,0.5),xlab="alpha_0",ylab="",cex.axis=1.5)
abline(h=0,lty=2)
lines(tau,V,"h",lwd=3,col="red")
title("Weights and support points for a simulated trajectory of a DP")
x11()
curve(dnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,ylim=c(0,0.5),xlab="alpha_0",ylab="",cex.axis=1.5)
abline(h=0,lty=2)
lines(tau,V,"h",lwd=3,col="red")
title("Weights and support points for a simulated trajectory of a DP")
x11()
curve(dnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,ylim=c(0,0.5),xlab=" ",ylab="",cex.axis=1.5) #mean parameter alpha_0 (in magenta)
abline(h=0,lty=2)
lines(tau,V,"h",lwd=3,col="red")
title("Weights and support points for a simulated trajectory of a DP")
sort(V) # When a is SMALL, there is one SINGLE LARGE weight (about 1);
X11()
oth <- order(tau)
#postscript("trajectories_a1.eps")
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab=" ",ylab="",cex.axis=1.5) # in magenta la f.r. di alpha_0, la misura media
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
title("Simulated trajectory of the DP distribution function")
a=0.5
M <- 750 # livello di troncamento   M=500
Y <- vector(length=M) ##Il vettore V conterrà le v.a. beta della costruzione stick breaking
tau <-  vector(length=M) ## i punti di supporto della mpa processo di Dirichlet - PRIMA li abbiamo chiamati theta
V <- vector(length=M)
# Simulo
Y <- rbeta(M,1,a) #simulo le beta
tau <- rnorm(M,0,1) # simulo i tau in modo iid da alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##Valuto la somma dei V
V <- V/sum(V)  ## Rinormalizzo i V
a=0.5
M <- 750 # livello di troncamento   M=500
Y <- vector(length=M) ##Il vettore V conterrà le v.a. beta della costruzione stick breaking
tau <-  vector(length=M) ## i punti di supporto della mpa processo di Dirichlet - PRIMA li abbiamo chiamati theta
V <- vector(length=M)
# Simulo
Y <- rbeta(M,1,a) #simulo le beta
tau <- rnorm(M,0,1) # simulo i tau in modo iid da alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##Valuto la somma dei V
V <- V/sum(V)  ## Rinormalizzo i V
#postscript("comparison.eps")
x11()
par(mfrow=c(1,3))
oth <- order(tau)
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab="",ylab="",cex.axis=1.5)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
title("Total mass a=0.5")
gra=gray(1:100/100)
gra=rep(gra,10)
for(j in 1:50){   #for(j in 1:100){
set.seed(j)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T)
a=5
M <- 750 # truncation level   M=500
Y <- vector(length=M) ## beta proportions in the stick breaking construction
tau <-  vector(length=M) ## support points
V <- vector(length=M)
# Simulation
Y <- rbeta(M,1,a) #simulation of the beta rvs
tau <- rnorm(M,0,1) # tau_i iid from alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##summation of the weights V
V <- V/sum(V)  ## Rinormalization of the weights
a=0.5
M <- 750 # livello di troncamento   M=500
Y <- vector(length=M) ##Il vettore V conterrà le v.a. beta della costruzione stick breaking
tau <-  vector(length=M) ## i punti di supporto della mpa processo di Dirichlet - PRIMA li abbiamo chiamati theta
V <- vector(length=M)
# Simulo
Y <- rbeta(M,1,a) #simulo le beta
tau <- rnorm(M,0,1) # simulo i tau in modo iid da alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##Valuto la somma dei V
V <- V/sum(V)  ## Rinormalizzo i V
#postscript("comparison.eps")
x11()
par(mfrow=c(1,3))
oth <- order(tau)
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab="",ylab="",cex.axis=1.5)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
title("Total mass a=0.5")
gra=gray(1:100/100)
gra=rep(gra,10)
for(j in 1:50){   #for(j in 1:100){
set.seed(j)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T)
##
a=5
M <- 750 # truncation level   M=500
Y <- vector(length=M) ## beta proportions in the stick breaking construction
tau <-  vector(length=M) ## support points
V <- vector(length=M)
# Simulation
Y <- rbeta(M,1,a) #simulation of the beta rvs
tau <- rnorm(M,0,1) # tau_i iid from alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##summation of the weights V
V <- V/sum(V)  ## Rinormalization of the weights
oth <- order(tau)
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab="",ylab="",cex.axis=1.5)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
title("Total mass a=5")
gra=gray(1:100/100)
gra=rep(gra,10)
for(j in 1:50){   #for(j in 1:100){
set.seed(j)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T)
##
a=50
M <- 750 # livello di troncamento   M=500
Y <- vector(length=M) ##Il vettore V conterrà le v.a. beta della costruzione stick breaking
tau <-  vector(length=M) ## i punti di supporto della mpa processo di Dirichlet - PRIMA li abbiamo chiamati theta
V <- vector(length=M)
# Simulo
Y <- rbeta(M,1,a) #simulo le beta
tau <- rnorm(M,0,1) # simulo i tau in modo iid da alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##Valuto la somma dei V
V <- V/sum(V)  ## Rinormalizzo i V
oth <- order(tau)
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab="",ylab="",cex.axis=1.5)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
title("Total mass a=50")
gra=gray(1:100/100)
gra=rep(gra,10)
for(j in 1:50){    #for(j in 1:100){
set.seed(j)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T,xlab="",ylab="",cex.axis=1.5)
a=1
M <- 100 # livello di troncamento   M=30
Y <- vector(length=M) ##Il vettore V conterrà le v.a. beta della costruzione stick breaking
tau <-  vector(length=M) ## i punti di supporto della mpa processo di Dirichlet - PRIMA li abbiamo chiamati theta
V <- vector(length=M)
# Simulo
Y <- rbeta(M,1,a) #simulo le beta
tau <- rnorm(M,0,1) # simulo i tau in modo iid da alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##Valuto la somma dei V
V <- V/sum(V)
round(V,4)
n=10 ## sample size
theta<-vector(length=n)
index <-vector(length=n)
for(j in 1:n){
index[j] <- sample(1:M,size=1,prob=V)
theta[j]=tau[index[j]]
}
index
theta
unique(theta)
help("sample")
vect = c(1,2,3,1)
which(vect=='1')
vect(which(vect=='1'))
vect[which(vect=='1')]
help(plot)
setwd("C:/Users/admin/Documents/R/Project_BS/BayesianProject")
load('Simulated_WP.RData')
data<-data1
rm(data1)
>>>>>>> 5eb24240ac694d43fa5d6b1e01903ac4327a2d7b
gibbs_loss <- function(n_clust, centroids, label ,data){
res = rep(0,n_clust)
for (k in 1:n_clust){
for (i in 1:n){
if (label[i] == k){
sum_partial = alpha_Mahalanobis(alpha,data[i,],centroids[k,],eig$values,eig$vectors)
res[k] = res[k] + sum_partial
}
}
}
tot = sum(res)
return(tot)
}
fda_clustering_mahalanobis <- function(n_clust, alpha, eig, toll,data){
n <- dim(data)[1]
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
vect_sample <- 1:n
y0[1] <- sample(vect_sample,1)
for (k in 2:n_clust) {
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
# vector of labels
c_lab <- rep(0,n)
# eigenvalues and eigenfunctions for the alpha-mahalanobis function
values <- eig$values
vectors <- eig$vectors
Mahalanobis_Distance <- matrix(0, nrow = n, ncol = n)
for (i in 1:n){
for (j in 1:n){
Mahalanobis_Distance[i,j] <- alpha_Mahalanobis(alpha,data[i,],data[j,],values,vectors)
}
}
# i-th unit belongs to cluster_k if the distance(centroid_k,i-th unit) is the smallest one
Maha_dis <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis[i,k] <- Mahalanobis_Distance[i,y0[k]]
}
index <- which.min(Maha_dis[i,])
c_lab[i] <- index
}
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_random[k,] <- data[y0[k],]
}
loss_value1 <- gibbs_loss(n_clust = n_clust, centroids = centroids_random, label = c_lab, data = data)
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
while(abs(loss_value1 - loss_value2) >= toll){
c_lab <- rep(0,n)
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,],values,vectors)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
loss_value1 <- loss_value2
for (k in 1:n_clust){
<<<<<<< HEAD
centroids_mean[k,] <- colMeans(data[which(c_lab=='k'),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
}
return(list("label" = c_lab, "centroids" = centroids_mean))
}
# Application on the simulated data
k <- 2
clust <- fda_clustering_mahalanobis(n_clust = k, alpha = alpha, eig = eig, toll = 1e-6, data = data)
fda_clustering_mahalanobis <- function(n_clust, alpha, eig, toll,data){
n <- dim(data)[1]
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
vect_sample <- 1:n
y0[1] <- sample(vect_sample,1)
for (k in 2:n_clust) {
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
# vector of labels
c_lab <- rep(0,n)
# eigenvalues and eigenfunctions for the alpha-mahalanobis function
values <- eig$values
vectors <- eig$vectors
Mahalanobis_Distance <- matrix(0, nrow = n, ncol = n)
for (i in 1:n){
for (j in 1:n){
Mahalanobis_Distance[i,j] <- alpha_Mahalanobis(alpha,data[i,],data[j,],values,vectors)
}
}
# i-th unit belongs to cluster_k if the distance(centroid_k,i-th unit) is the smallest one
Maha_dis <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis[i,k] <- Mahalanobis_Distance[i,y0[k]]
}
index <- which.min(Maha_dis[i,])
c_lab[i] <- index
}
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_random[k,] <- data[y0[k],]
}
loss_value1 <- gibbs_loss(n_clust = n_clust, centroids = centroids_random, label = c_lab, data = data)
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
while(abs(loss_value1 - loss_value2) >= toll){
c_lab <- rep(0,n)
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,],values,vectors)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
loss_value1 <- loss_value2
for (k in 1:n_clust){
=======
>>>>>>> 5eb24240ac694d43fa5d6b1e01903ac4327a2d7b
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
}
return(list("label" = c_lab, "centroids" = centroids_mean))
}
<<<<<<< HEAD
# Application on the simulated data
=======
>>>>>>> 5eb24240ac694d43fa5d6b1e01903ac4327a2d7b
k <- 2
clust <- fda_clustering_mahalanobis(n_clust = k, alpha = alpha, eig = eig, toll = 1e-6, data = data)
c_opt <- clust$label
c1 <- clust$centroids[1,]
c2 <- clust$centroids[2,]
show(c_opt)  #label switching
<<<<<<< HEAD
# Theoretical optimal plot vs clustering plot
=======
>>>>>>> 5eb24240ac694d43fa5d6b1e01903ac4327a2d7b
data1 <- data[which(c_opt=='1'),]
data2 <- data[which(c_opt=='2'),]
x11()
par(mfrow = c(1,2))
plot(time,data[1,],type = 'l', ylim = c(-2,7.5), col = 'firebrick2', lwd = 2, main = "Main and contaminated processes")
for(i in 2:(n-c)){
lines(time,data[i,],type = 'l', col = 'firebrick2',lwd = 2)
}
for (i in (n-c+1):n){
lines(time,data[i,],type = 'l', col = 'blue', lwd = 2)
}
plot(time,data1[1,],type = 'l', ylim = c(-2,7.5), col = 'firebrick2', lwd = 2, main = "Clustered data")
for (i in 2:dim(data1)[1]){
lines(time,data1[i,],type = 'l', col = 'firebrick2',lwd = 2)
}
for (i in 1:dim(data2)[1]){
lines(time,data2[i,],type = 'l', col = 'blue',lwd = 2)
}
<<<<<<< HEAD
lines(time,c1,type = 'l', lwd = 4)
lines(time,c2,type = 'l', lwd = 4)
rm(data1)
rm(data2)
# Application on the simulated data
k <- 3
clust <- fda_clustering_mahalanobis(n_clust = k, alpha = alpha, eig = eig, toll = 1e-6, data = data)
c_opt <- clust$label
c1 <- clust$centroids[1,]
c2 <- clust$centroids[2,]
show(c_opt)  #label switching
setwd("C:/Users/pietr/Desktop/Bayesian Statistics/Progetto/dati/BayesianProject")
load('Simulated_WP.RData')
data<-data1
rm(data1)
#### Loss function ####
#centroids è una matrice avente n_clust righe e ogni riga è un centroide
gibbs_loss <- function(n_clust, centroids, label ,data){
res = rep(0,n_clust)
for (k in 1:n_clust){
for (i in 1:n){
if (label[i] == k){
sum_partial = alpha_Mahalanobis(alpha,data[i,],centroids[k,],eig$values,eig$vectors)
res[k] = res[k] + sum_partial
}
}
}
tot = sum(res)
return(tot)
}
fda_clustering_mahalanobis <- function(n_clust, alpha, eig, toll,data){
n <- dim(data)[1]
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
vect_sample <- 1:n
y0[1] <- sample(vect_sample,1)
for (k in 2:n_clust) {
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
# vector of labels
c_lab <- rep(0,n)
# eigenvalues and eigenfunctions for the alpha-mahalanobis function
values <- eig$values
vectors <- eig$vectors
Mahalanobis_Distance <- matrix(0, nrow = n, ncol = n)
for (i in 1:n){
for (j in 1:n){
Mahalanobis_Distance[i,j] <- alpha_Mahalanobis(alpha,data[i,],data[j,],values,vectors)
}
}
# i-th unit belongs to cluster_k if the distance(centroid_k,i-th unit) is the smallest one
Maha_dis <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis[i,k] <- Mahalanobis_Distance[i,y0[k]]
}
index <- which.min(Maha_dis[i,])
c_lab[i] <- index
}
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_random[k,] <- data[y0[k],]
}
loss_value1 <- gibbs_loss(n_clust = n_clust, centroids = centroids_random, label = c_lab, data = data)
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = dim(data)[2])
for (k in 1:n_clust){
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
while(abs(loss_value1 - loss_value2) >= toll){
c_lab <- rep(0,n)
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,],values,vectors)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
loss_value1 <- loss_value2
for (k in 1:n_clust){
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean, label = c_lab, data = data)
}
return(list("label" = c_lab, "centroids" = centroids_mean))
}
# Application on the simulated data
k <- 3
clust <- fda_clustering_mahalanobis(n_clust = k, alpha = alpha, eig = eig, toll = 1e-6, data = data)
c_opt <- clust$label
c1 <- clust$centroids[1,]
c2 <- clust$centroids[2,]
c3 <- clust$centroids[3,]
show(c_opt)  #label switching
# Theoretical optimal plot vs clustering plot
data1 <- data[which(c_opt=='1'),]
data2 <- data[which(c_opt=='2'),]
data3 <- data[which(c_opt=='3'),]
x11()
par(mfrow = c(1,2))
plot(time,data[1,],type = 'l', ylim = c(-2,7.5), col = 'firebrick2', lwd = 2, main = "Main and contaminated processes")
for(i in 2:(n-c)){
lines(time,data[i,],type = 'l', col = 'firebrick2',lwd = 2)
}
for (i in (n-c+1):n){
lines(time,data[i,],type = 'l', col = 'blue', lwd = 2)
}
plot(time,data1[1,],type = 'l', ylim = c(-2,7.5), col = 'firebrick2', lwd = 2, main = "Clustered data")
for (i in 2:dim(data1)[1]){
lines(time,data1[i,],type = 'l', col = 'firebrick2',lwd = 2)
}
for (i in 1:dim(data2)[1]){
lines(time,data2[i,],type = 'l', col = 'blue',lwd = 2)
}
for (i in 1:dim(data3)[1]){
lines(time,data3[i,],type = 'l', col = 'forestgreen',lwd = 2)
}
lines(time,c1,type = 'l', lwd = 4)
lines(time,c2,type = 'l', lwd = 4)
lines(time,c3,type = 'l', lwd = 4)
rm(data1)
rm(data2)
rm(data3)
# Application on the simulated data
k <- 2
clust <- fda_clustering_mahalanobis(n_clust = k, alpha = alpha, eig = eig, toll = 1e-6, data = data)
c_opt <- clust$label
c1 <- clust$centroids[1,]
c2 <- clust$centroids[2,]
show(c_opt)  #label switching
# Theoretical optimal plot vs clustering plot
data1 <- data[which(c_opt=='1'),]
data2 <- data[which(c_opt=='2'),]
x11()
par(mfrow = c(1,2))
plot(time,data[1,],type = 'l', ylim = c(-2,7.5), col = 'firebrick2', lwd = 2, main = "Main and contaminated processes")
for(i in 2:(n-c)){
lines(time,data[i,],type = 'l', col = 'firebrick2',lwd = 2)
}
for (i in (n-c+1):n){
lines(time,data[i,],type = 'l', col = 'blue', lwd = 2)
}
plot(time,data1[1,],type = 'l', ylim = c(-2,7.5), col = 'firebrick2', lwd = 2, main = "Clustered data")
for (i in 2:dim(data1)[1]){
lines(time,data1[i,],type = 'l', col = 'firebrick2',lwd = 2)
}
for (i in 1:dim(data2)[1]){
lines(time,data2[i,],type = 'l', col = 'blue',lwd = 2)
}
#for (i in 1:dim(data3)[1]){
#lines(time,data3[i,],type = 'l', col = 'forestgreen',lwd = 2)
#}
lines(time,c1,type = 'l', lwd = 4)
lines(time,c2,type = 'l', lwd = 4)
rm(data1)
rm(data2)
rm(data3)
# Theoretical optimal plot vs clustering plot SMOOTHED
data1 <- f.data_alpha_sim[which(c_opt=='1'),]
data2 <- f.data_alpha_sim[which(c_opt=='2'),]
x11()
par(mfrow = c(1,2))
plot(time,f.data_alpha_sim[1,],type = 'l', ylim = c(-2,7.5), col = 'firebrick2', lwd = 2, main = "Smooth processes")
for(i in 2:(n-c)){
lines(time,f.data_alpha_sim[i,],type = 'l', col = 'firebrick2',lwd = 2)
}
for (i in (n-c+1):n){
lines(time,f.data_alpha_sim[i,],type = 'l', col = 'blue', lwd = 2)
}
plot(time,data1[1,],type = 'l', ylim = c(-2,7.5), col = 'firebrick2', lwd = 2, main = "Clustered smoothed data")
for (i in 2:dim(data1)[1]){
lines(time,data1[i,],type = 'l', col = 'firebrick2',lwd = 2)
}
for (i in 1:dim(data2)[1]){
lines(time,data2[i,],type = 'l', col = 'blue',lwd = 2)
}
lines(time,f_alpha_approx(c1,alpha,eig$values,eig$vectors),type = 'l', lwd = 4)
lines(time,f_alpha_approx(c2,alpha,eig$values,eig$vectors),type = 'l', lwd = 4)
rm(data1)
rm(data2)
setwd("C:/Users/pietr/Desktop/Bayesian Statistics/Progetto/dati/BayesianProject")
load('Simulated_WP.RData')
data<-data1
rm(data1)
=======
>>>>>>> 5eb24240ac694d43fa5d6b1e01903ac4327a2d7b
