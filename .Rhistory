vectors <- eig$vectors
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
vect_sample <- 1:n
# while cycle checks that there are no single unit clusters in the intial step
flag_1 <- 1
while (flag_1 != 0) {
flag_1 <- 0
y0[1] <- sample(vect_sample,1)
for (k in 2:n_clust) {
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
Mahalanobis_Distance <- matrix(0, nrow = n, ncol = n)
for (i in 1:n){
for (j in 1:n){
Mahalanobis_Distance[i,j] <- alpha_Mahalanobis(alpha,data[i,],data[j,]
,values,vectors)
}
}
# i-th unit belongs to cluster_k if the distance(centroid_k,i-th unit) is the smallest one
Maha_dis <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis[i,k] <- Mahalanobis_Distance[i,y0[k]]
}
index <- which.min(Maha_dis[i,])
c_lab[i] <- index
}
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = t_points)
for (k in 1:n_clust){
centroids_random[k,] <- data[y0[k],]
}
for (k in 1:n_clust){
if ( is.null(dim(data[which(c_lab ==k),])[1] == TRUE )) {
flag_1 <- flag_1 + 1 # flag gets updated if there are single unit clusters
}
}
}
c_lab
loss_value1 <- gibbs_loss(n_clust = n_clust, centroids = centroids_random,
label = c_lab, eig = eig,data = data)
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = t_points)
for (k in 1:n_clust){
if (is.null(dim(data[which(c_lab==k),])[1]) == TRUE) {
centroids_mean[k,] <- data[which(c_lab ==k),]
}
else
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean,
label = c_lab, eig = eig, data = data)
values_matrix <- matrix (0, nrow = t_points, ncol = n_clust)
vector_matrix <- matrix (0, nrow = (n_clust*t_points), ncol = t_points )
values_k <- rep(0,t_points)
vector_k <- matrix (0, nrow = t_points, ncol = t_points)
# while cycle to check there are no single unit clusters in the intermediate steps
flag_2 <- 1
while (flag_2 != 0) {
flag_2 <- 0
for (k in 1:n_clust){
data_k <- data[which(c_lab ==k),]
cov_k <- cov(data_k)
eig_k <- eigen(cov_k)
values_matrix[,k] <- eig_k$values
vector_matrix[((k-1)*t_points + 1):(k*t_points),] <- eig_k$vectors
}
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
values_k <- values_matrix[,k]
vector_k <- vector_matrix[((k-1)*t_points + 1):(k*t_points),]
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,]
,values_k,vector_k)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
for (k in 1:n_clust){
if (is.null(dim(data[which(c_lab==k),])[1]) == TRUE) {
centroids_mean[k,] <- data[which(c_lab ==k),]
}
else
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
flag_2 <- 0
for (k in 1:n_clust){
if ( is.null(dim(data[which(c_lab ==k),])[1] == TRUE )) {
flag_2 <- flag_2 + 1 # flag gets updated if there are single unit clusters
}
}
}
loss_value2 <- gibbs_loss_k(n_clust = n_clust, centroids = centroids_mean,
label = c_lab, values_matrix, vector_matrix, data = data)
loss_value1 <- loss_value2
# while cycle to check there are no single unit clusters in the intermediate steps
flag_2 <- 1
while (flag_2 != 0) {
flag_2 <- 0
for (k in 1:n_clust){
data_k <- data[which(c_lab ==k),]
cov_k <- cov(data_k)
eig_k <- eigen(cov_k)
values_matrix[,k] <- eig_k$values
vector_matrix[((k-1)*t_points + 1):(k*t_points),] <- eig_k$vectors
}
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
values_k <- values_matrix[,k]
vector_k <- vector_matrix[((k-1)*t_points + 1):(k*t_points),]
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,]
,values_k,vector_k)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
for (k in 1:n_clust){
if (is.null(dim(data[which(c_lab==k),])[1]) == TRUE) {
centroids_mean[k,] <- data[which(c_lab ==k),]
}
else
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
flag_2 <- 0
for (k in 1:n_clust){
if ( is.null(dim(data[which(c_lab ==k),])[1] == TRUE )) {
flag_2 <- flag_2 + 1 # flag gets updated if there are single unit clusters
}
}
}
loss_value2 <- gibbs_loss_k(n_clust = n_clust, centroids = centroids_mean,
label = c_lab, values_matrix, vector_matrix, data = data)
loss_value1 <- loss_value2
# while cycle to check there are no single unit clusters in the intermediate steps
flag_2 <- 1
while (flag_2 != 0) {
flag_2 <- 0
for (k in 1:n_clust){
data_k <- data[which(c_lab ==k),]
cov_k <- cov(data_k)
eig_k <- eigen(cov_k)
values_matrix[,k] <- eig_k$values
vector_matrix[((k-1)*t_points + 1):(k*t_points),] <- eig_k$vectors
}
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
values_k <- values_matrix[,k]
vector_k <- vector_matrix[((k-1)*t_points + 1):(k*t_points),]
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,]
,values_k,vector_k)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
for (k in 1:n_clust){
if (is.null(dim(data[which(c_lab==k),])[1]) == TRUE) {
centroids_mean[k,] <- data[which(c_lab ==k),]
}
else
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
flag_2 <- 0
for (k in 1:n_clust){
if ( is.null(dim(data[which(c_lab ==k),])[1] == TRUE )) {
flag_2 <- flag_2 + 1 # flag gets updated if there are single unit clusters
}
}
}
c_lab
loss_value2 <- gibbs_loss_k(n_clust = n_clust, centroids = centroids_mean,
label = c_lab, values_matrix, vector_matrix, data = data)
##### Setup ####
setwd("C:/Users/pietr/Desktop/Bayesian Statistics/Progetto/dati/BayesianProject")
load("functional_WP.RData")
library(fda.usc)
library(fda)
library(fields)
# Modified for eigenvalues/eigenvectros related to the clusters. Both functions are called in the code
gibbs_loss_k <- function(n_clust, centroids, label , values_matrix, vector_matrix, data){
t_points <- dim(data)[2]
n <- dim(data)[1]
res = rep(0,n_clust)
sum_partial <- 0
values_k <- rep(0,t_points)
vector_k <- matrix(0, t_points, t_points)
for (k in 1:n_clust){
values_k <- values_matrix[,k]
vector_k <- vector_matrix[((k-1)*t_points + 1):(k*t_points),]
for (i in 1:n){
if (label[i] == k){
sum_partial = alpha_Mahalanobis(alpha,data[i,],centroids[k,],eig$values,eig$vectors)
res[k] = res[k] + sum_partial
}
}
}
tot = sum(res)
return(tot)
}
fda_clustering_mahalanobis_updated <- function(n_clust, alpha, cov_matrix, toll,data){
t_points <- dim(data)[2]
n <- dim(data)[1]
# vector of labels
c_lab <- rep(0,n)
# eigenvalues and eigenfunctions for the alpha-mahalanobis function
eig <- eigen(cov_matrix)
values <- eig$values
vectors <- eig$vectors
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
vect_sample <- 1:n
# while cycle checks that there are no single unit clusters in the intial step
flag_1 <- 1
while (flag_1 != 0) {
flag_1 <- 0
y0[1] <- sample(vect_sample,1)
for (k in 2:n_clust) {
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
Mahalanobis_Distance <- matrix(0, nrow = n, ncol = n)
for (i in 1:n){
for (j in 1:n){
Mahalanobis_Distance[i,j] <- alpha_Mahalanobis(alpha,data[i,],data[j,]
,values,vectors)
}
}
# i-th unit belongs to cluster_k if the distance(centroid_k,i-th unit) is the smallest one
Maha_dis <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis[i,k] <- Mahalanobis_Distance[i,y0[k]]
}
index <- which.min(Maha_dis[i,])
c_lab[i] <- index
}
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = t_points)
for (k in 1:n_clust){
centroids_random[k,] <- data[y0[k],]
}
for (k in 1:n_clust){
if ( is.null(dim(data[which(c_lab ==k),])[1] == TRUE )) {
flag_1 <- flag_1 + 1 # flag gets updated if there are single unit clusters
}
}
}
loss_value1 <- gibbs_loss(n_clust = n_clust, centroids = centroids_random,
label = c_lab, eig = eig,data = data)
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = t_points)
for (k in 1:n_clust){
if (is.null(dim(data[which(c_lab==k),])[1]) == TRUE) {
centroids_mean[k,] <- data[which(c_lab ==k),]
}
else
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean,
label = c_lab, eig = eig, data = data)
values_matrix <- matrix (0, nrow = t_points, ncol = n_clust)
vector_matrix <- matrix (0, nrow = (n_clust*t_points), ncol = t_points )
values_k <- rep(0,t_points)
vector_k <- matrix (0, nrow = t_points, ncol = t_points)
while(abs(loss_value1 - loss_value2) >= toll){
loss_value1 <- loss_value2
# while cycle to check there are no single unit clusters in the intermediate steps
flag_2 <- 1
while (flag_2 != 0) {
flag_2 <- 0
for (k in 1:n_clust){
data_k <- data[which(c_lab ==k),]
cov_k <- cov(data_k)
eig_k <- eigen(cov_k)
values_matrix[,k] <- eig_k$values
vector_matrix[((k-1)*t_points + 1):(k*t_points),] <- eig_k$vectors
}
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
values_k <- values_matrix[,k]
vector_k <- vector_matrix[((k-1)*t_points + 1):(k*t_points),]
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,]
,values_k,vector_k)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
for (k in 1:n_clust){
if (is.null(dim(data[which(c_lab==k),])[1]) == TRUE) {
centroids_mean[k,] <- data[which(c_lab ==k),]
}
else
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
flag_2 <- 0
for (k in 1:n_clust){
if ( is.null(dim(data[which(c_lab ==k),])[1] == TRUE )) {
flag_2 <- flag_2 + 1 # flag gets updated if there are single unit clusters
}
}
}
loss_value2 <- gibbs_loss_k(n_clust = n_clust, centroids = centroids_mean,
label = c_lab, values_matrix, vector_matrix, data = data)
}
return(list("label" = c_lab, "centroids" = centroids_mean, "loss" = loss_value2))
}
f.data.clust <- fda_clustering_mahalanobis_updated(n_clust = 2, alpha = 10000,
cov_matrix = cov(f.data$ausxSL$data),
toll = 1e-2, data = f.data$ausxSL$data)
gibbs_loss <- function(n_clust, centroids, eig ,label ,data){
res = rep(0,n_clust)
for (k in 1:n_clust){
for (i in 1:n){
if (label[i] == k){
sum_partial = alpha_Mahalanobis(alpha,data[i,],centroids[k,],eig$values,eig$vectors)
res[k] = res[k] + sum_partial
}
}
}
tot = sum(res)
return(tot)
}
# Modified for eigenvalues/eigenvectros related to the clusters. Both functions are called in the code
gibbs_loss_k <- function(n_clust, centroids, label , values_matrix, vector_matrix, data){
t_points <- dim(data)[2]
n <- dim(data)[1]
res = rep(0,n_clust)
sum_partial <- 0
values_k <- rep(0,t_points)
vector_k <- matrix(0, t_points, t_points)
for (k in 1:n_clust){
values_k <- values_matrix[,k]
vector_k <- vector_matrix[((k-1)*t_points + 1):(k*t_points),]
for (i in 1:n){
if (label[i] == k){
sum_partial = alpha_Mahalanobis(alpha,data[i,],centroids[k,],eig$values,eig$vectors)
res[k] = res[k] + sum_partial
}
}
}
tot = sum(res)
return(tot)
}
setwd("C:/Users/pietr/Desktop/Bayesian Statistics/Progetto/dati/BayesianProject")
load("functional_WP.RData")
library(fda.usc)
library(fda)
library(fields)
gibbs_loss <- function(n_clust, centroids, eig ,label ,data){
res = rep(0,n_clust)
for (k in 1:n_clust){
for (i in 1:n){
if (label[i] == k){
sum_partial = alpha_Mahalanobis(alpha,data[i,],centroids[k,],eig$values,eig$vectors)
res[k] = res[k] + sum_partial
}
}
}
tot = sum(res)
return(tot)
}
# Modified for eigenvalues/eigenvectros related to the clusters. Both functions are called in the code
gibbs_loss_k <- function(n_clust, centroids, label , values_matrix, vector_matrix, data){
t_points <- dim(data)[2]
n <- dim(data)[1]
res = rep(0,n_clust)
sum_partial <- 0
values_k <- rep(0,t_points)
vector_k <- matrix(0, t_points, t_points)
for (k in 1:n_clust){
values_k <- values_matrix[,k]
vector_k <- vector_matrix[((k-1)*t_points + 1):(k*t_points),]
for (i in 1:n){
if (label[i] == k){
sum_partial = alpha_Mahalanobis(alpha,data[i,],centroids[k,],eig$values,eig$vectors)
res[k] = res[k] + sum_partial
}
}
}
tot = sum(res)
return(tot)
}
fda_clustering_mahalanobis_updated <- function(n_clust, alpha, cov_matrix, toll,data){
t_points <- dim(data)[2]
n <- dim(data)[1]
# vector of labels
c_lab <- rep(0,n)
# eigenvalues and eigenfunctions for the alpha-mahalanobis function
eig <- eigen(cov_matrix)
values <- eig$values
vectors <- eig$vectors
# index of each centroid randomly defined through sampling
y0 <- rep(0,n_clust)
vect_sample <- 1:n
# while cycle checks that there are no single unit clusters in the intial step
flag_1 <- 1
while (flag_1 != 0) {
flag_1 <- 0
y0[1] <- sample(vect_sample,1)
for (k in 2:n_clust) {
value <- y0[k-1]
for (i in 1:length(vect_sample)){
if (vect_sample[i] == value)
t = i
}
vect_sample <- vect_sample[-t]
y0[k] <- sample(vect_sample,1)
}
Mahalanobis_Distance <- matrix(0, nrow = n, ncol = n)
for (i in 1:n){
for (j in 1:n){
Mahalanobis_Distance[i,j] <- alpha_Mahalanobis(alpha,data[i,],data[j,]
,values,vectors)
}
}
# i-th unit belongs to cluster_k if the distance(centroid_k,i-th unit) is the smallest one
Maha_dis <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
Maha_dis[i,k] <- Mahalanobis_Distance[i,y0[k]]
}
index <- which.min(Maha_dis[i,])
c_lab[i] <- index
}
# define the matrix of the centroids (random centroids)
centroids_random <- matrix(0,nrow = n_clust,ncol = t_points)
for (k in 1:n_clust){
centroids_random[k,] <- data[y0[k],]
}
for (k in 1:n_clust){
if ( is.null(dim(data[which(c_lab ==k),])[1] == TRUE )) {
flag_1 <- flag_1 + 1 # flag gets updated if there are single unit clusters
}
}
}
loss_value1 <- gibbs_loss(n_clust = n_clust, centroids = centroids_random,
label = c_lab, eig = eig,data = data)
# update each centroid as the mean of the clusters data
centroids_mean<-matrix(0,nrow = n_clust, ncol = t_points)
for (k in 1:n_clust){
if (is.null(dim(data[which(c_lab==k),])[1]) == TRUE) {
centroids_mean[k,] <- data[which(c_lab ==k),]
}
else
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
loss_value2 <- gibbs_loss(n_clust = n_clust, centroids = centroids_mean,
label = c_lab, eig = eig, data = data)
values_matrix <- matrix (0, nrow = t_points, ncol = n_clust)
vector_matrix <- matrix (0, nrow = (n_clust*t_points), ncol = t_points )
values_k <- rep(0,t_points)
vector_k <- matrix (0, nrow = t_points, ncol = t_points)
while(abs(loss_value1 - loss_value2) >= toll){
loss_value1 <- loss_value2
# while cycle to check there are no single unit clusters in the intermediate steps
flag_2 <- 1
while (flag_2 != 0) {
flag_2 <- 0
for (k in 1:n_clust){
data_k <- data[which(c_lab ==k),]
cov_k <- cov(data_k)
eig_k <- eigen(cov_k)
values_matrix[,k] <- eig_k$values
vector_matrix[((k-1)*t_points + 1):(k*t_points),] <- eig_k$vectors
}
Maha_dis_k <- matrix(0,nrow=n, ncol=n_clust)
for (i in 1:n){
for (k in 1:n_clust) {
values_k <- values_matrix[,k]
vector_k <- vector_matrix[((k-1)*t_points + 1):(k*t_points),]
Maha_dis_k[i,k] <- alpha_Mahalanobis(alpha,centroids_mean[k,],data[i,]
,values_k,vector_k)
}
index <- which.min(Maha_dis_k[i,])
c_lab[i] <- index
}
for (k in 1:n_clust){
if (is.null(dim(data[which(c_lab==k),])[1]) == TRUE) {
centroids_mean[k,] <- data[which(c_lab ==k),]
}
else
centroids_mean[k,] <- colMeans(data[which(c_lab==k),])
}
flag_2 <- 0
for (k in 1:n_clust){
if ( is.null(dim(data[which(c_lab ==k),])[1] == TRUE )) {
flag_2 <- flag_2 + 1 # flag gets updated if there are single unit clusters
}
}
}
loss_value2 <- gibbs_loss_k(n_clust = n_clust, centroids = centroids_mean,
label = c_lab, values_matrix, vector_matrix, data = data)
}
return(list("label" = c_lab, "centroids" = centroids_mean, "loss" = loss_value2))
}
f.data.clust <- fda_clustering_mahalanobis_updated(n_clust = 2, alpha = 10000,
cov_matrix = cov(f.data$ausxSL$data),
toll = 1e-2, data = f.data$ausxSL$data)
